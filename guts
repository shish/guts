#!/usr/bin/python

import os
import sys
import argparse
import hashlib
import random
import time
from urlparse import urlparse


def random_hash():
    base = time.time()
    base = base + random.randint(0, sys.maxint)
    return hashlib.sha1(str(base)).hexdigest()


def read_history(filename):
    lines = file(filename).readlines()

    commits = []
    commit = None
    for line in lines:
        if line.strip() == "":
            continue
        k, v = line.strip().split(":", 1)
        if k == "commit":
            if commit:
                commits.append(commit)
            commit = Commit()
            commit.hash = v
            commit.changes = []
        if not commit:
            continue
        if k == "author":
            commit.author = v
        if k == "date":
            commit.date = v
        if k == "add":
            commit.changes.append(Add(line.strip()))
        if k == "modify":
            commit.changes.append(Modify(line.strip()))
        if k == "delete":
            commit.changes.append(Delete(line.strip()))
    if commit:
        commits.append(commit)

    return commits


def calc_working_tree():
    files = {}
    for commit in read_history(".gut/history"):
        for ch in commit.changes:
            if ch.type == "add":
                files[ch.filename] = File(ch.filename, ch.mode, ch.mtime, ch.object)
            if ch.type == "modify":
                files[ch.filename].old_objects.append(files[ch.filename].object)
                files[ch.filename].mode = ch.mode
                files[ch.filename].mtime = ch.mtime
                files[ch.filename].object = ch.object
            if ch.type == "delete":
                files[ch.filename] = File(ch.filename, "000000", "0", "0" * 40)
    return files


def net_download(repo_url, repo_path, local_path):
    up = urlparse(repo_url)
    if up.scheme == "ssh":
        os.system('scp -q "%s:/%s" "%s"' % (up.hostname, up.path+"/"+repo_path, local_path))
    if up.scheme == "rsync+ssh":
        os.system('rsync "%s:/%s" "%s"' % (up.hostname, up.path+"/"+repo_path, local_path))


def net_upload(repo_url, local_path, repo_path):
    up = urlparse(repo_url)
    if up.scheme == "ssh":
        os.system('scp -q "%s" "%s:/%s"' % (local_path, up.hostname, up.path+"/"+repo_path))
    if up.scheme == "rsync+ssh":
        os.system('rsync "%s" "%s:/%s"' % (local_path, up.hostname, up.path+"/"+repo_path))


#######################################################################
# objects

class Commit(object):
    def __init__(self):
        self.author = None
        self.date = None
        self.hash = None
        self.changes = []

    def to_git(self):
        s = ""
        s = s + "commit %s\n" % self.hash
        s = s + "Author: %s\n" % self.author
        s = s + "Date:   %s\n" % self.date
        s = s + "\n"
        s = s + "    Automatic commit\n"
        s = s + "\n"
        for change in self.changes:
            s = s + change.to_git() + "\n"
        return s

    def to_gut(self):
        s = ""
        s = s + "revision:%s\n" % self.hash
        s = s + "user:%s\n" % self.author
        s = s + "timestamp:%s\n" % self.date
        for change in self.changes:
            s = s + change.to_gut() + "\n"
        return s

    def to_guts(self):
        s = ""
        s = s + "commit:%s\n" % self.hash
        s = s + "author:%s\n" % self.author
        s = s + "date:%s\n" % self.date
        for change in self.changes:
            s = s + change.to_guts() + "\n"
        return s


class Change(object):
    def __init__(self):
        pass

    def to_guts(self):
        return self.line


class Add(Change):
    def __init__(self, line):
        self.line = line
        self.type, self.filename, self.mode, self.mtime, self.object = line.split(":")

    def to_git(self):
        return ":%s %s %s %s %s  %s" % (
            "000000", self.mode, "0000000...", self.object[0:7] + "...",
            self.type[0].upper(),
            self.filename
        )

    def to_gut(self):
        return "added:" + self.filename


class Modify(Change):
    def __init__(self, line):
        self.line = line
        self.type, self.filename, self.mode, self.mtime, self.object = line.split(":")

    def to_git(self):
        return ":%s %s %s %s %s  %s" % (
            self.mode, self.mode, self.object[0:7] + "...", self.object[0:7] + "...",
            self.type[0].upper(),
            self.filename
        )

    def to_gut(self):
        return "edited:" + self.filename


class Delete(Change):
    def __init__(self, line):
        self.line = line
        self.type, self.filename = line.split(":")

    def to_git(self):
        return ":%s %s %s %s %s  %s" % (
            "100644", "000000", "1234567...", "0000000...",
            self.type[0].upper(),
            self.filename
        )

    def to_gut(self):
        return "deleted:" + self.filename


class File(object):
    def __init__(self, filename, mode, mtime, object):
        self.filename = filename
        self.mode = mode
        self.mtime = mtime
        self.object = object
        self.old_objects = []


#######################################################################
# commands

def identifier():
    print file(".gut/repo_id").read()


def size():
    os.system("du -cb ./ | grep total | cut -f 1")


def history_size():
    os.system("du -cb ./.gut/ | grep total | cut -f 1")


def current_revision():
    print hashlib.sha1(file(".gut/history").read()).hexdigest()


def has_remote_changes():
    remote_url = file(".gut/remote_url").read().strip()
    net_download(remote_url, "history", ".gut/tmp-remote-history")

    local = hashlib.sha1(file(".gut/history").read()).hexdigest()
    remote = hashlib.sha1(file(".gut/tmp-remote-history").read()).hexdigest()

    if local != remote:
        print "true"
        return 0
    else:
        print "false"
        return 1


def has_local_changes():
    files = calc_working_tree()

    # check known files for changes
    for n, wtfilename in enumerate(files):
        f = files[wtfilename]

        if (
            # if exists when it shouldn't
            (f.mode == "000000" and os.path.exists(f.filename)) or
            # it exists and should, but has been changed
            (f.mode != "000000" and (
                (not os.path.exists(f.filename)) or
                (int(os.stat(f.filename).st_mtime) != int(f.mtime)) or
                (os.stat(f.filename).st_mode != int(f.mode, 8)) or
                (hashlib.sha1(file(f.filename).read()).hexdigest() != f.object)
            ))
        ):
            print "true"
            return 0

    # check for new files
    actual_files = {}
    for dirpath, dirnames, filenames in os.walk("."):
        if ".gut" in dirpath:
            continue
        for filename in filenames:
            full_name = os.path.join(dirpath, filename)
            if full_name[0:2] == "./":
                full_name = full_name[2:]
            if full_name not in files:
                print "true"
                return 0

    print "false"
    return 1


def sync_up():
    remote_url = file(".gut/remote_url").read().strip()
    author = file(".gut/local_user").read().strip()

    known_files = calc_working_tree()
    actual_files = {}

    for dirpath, dirnames, filenames in os.walk("."):
        if ".gut" in dirpath:
            continue
        for filename in filenames:
            full_name = os.path.join(dirpath, filename)
            if full_name[0:2] == "./":
                full_name = full_name[2:]
            actual_files[full_name] = File(
                full_name,
                "%o" % os.stat(full_name).st_mode,
                str(int(os.stat(full_name).st_mtime)),
                hashlib.sha1(file(full_name).read()).hexdigest()
            )

    commit = Commit()
    commit.hash = random_hash()  # FIXME: hash(previous commit's hash + changes)
    commit.author = author
    commit.date = time.strftime("%Y-%m-%d %H:%M:%S") + " +0000"  # FIXME: timezone
    for f in actual_files:
        if f not in known_files:
            commit.changes.append(Add(":".join([
                "add",
                f,
                actual_files[f].mode,
                actual_files[f].mtime,
                actual_files[f].object,
            ])))
    for f in known_files:
        if f not in actual_files:
            if known_files[f].mode != "000000":
                commit.changes.append(Delete("delete:" + f))
    for f in known_files:
        if f in actual_files:
            if actual_files[f].object != known_files[f].object:
                commit.changes.append(Modify(":".join([
                    "modify",
                    f,
                    actual_files[f].mode,
                    actual_files[f].mtime,
                    actual_files[f].object,
                ])))

    if len(commit.changes) == 0:
        print "no changes to sync"
        return 0

    trim = False

    # TODO: lock repo
    for ch in commit.changes:
        if ch.type == "add" or ch.type == "modify":
            net_upload(remote_url, ch.filename, "objects/%s/%s" % (ch.object[0:2], ch.object))
        if ch.type == "modify" or ch.type == "delete":
            if trim:
                pass  # FIXME: delete file.old_objects[-1]

    h = file(".gut/history", "a")
    h.write("\n")
    h.write(commit.to_guts())
    h.close()

    net_upload(remote_url, ".gut/history", "history")
    # TODO: unlock repo

    if True:
        print "true"
        return 0
    else:
        print "false"
        return 1


def sync_down():
    remote_url = file(".gut/remote_url").read().strip()
    conflicts = False

    print "Downloading history"
    net_download(remote_url, "history", ".gut/history")

    print "Processing history"
    files = calc_working_tree()

    for n, wtfilename in enumerate(files):
        print >>sys.stderr, "%d%%" % int(n * 100 / len(files))
        print wtfilename
        wtfile = files[wtfilename]

        if wtfile.mode == "000000":
            if os.path.exists(wtfile.filename):
                os.unlink(wtfile.filename)
        else:
            current_hash = None
            if os.path.exists(wtfile.filename):
                current_hash = hashlib.sha1(file(wtfile.filename).read()).hexdigest()

            if wtfile.object != current_hash:
                if current_hash and current_hash not in wtfile.old_objects:
                    conflicts = True
                    os.rename(wtfile.filename, wtfile.filename + "." + str(time.time()))
                filedir = os.path.dirname(wtfile.filename)
                if filedir and not os.path.exists(filedir):
                    os.makedirs(filedir)
                net_download(remote_url, "objects/%s/%s" % (wtfile.object[0:2], wtfile.object), wtfile.filename)

            if os.stat(wtfile.filename).st_mode != int(wtfile.mode, 8):
                os.chmod(wtfile.filename, int(wtfile.mode, 8))

            if os.stat(wtfile.filename).st_mtime != int(wtfile.mtime):
                os.utime(wtfile.filename, (int(wtfile.mtime), int(wtfile.mtime)))

    if not conflicts:
        print "true"
        return 0
    else:
        print "false"
        return 1


def configure(target_folder, user, url):
    if not target_folder:
        if os.path.exists("./.gut"):
            target_folder = "./"
        else:
            print "target_folder not specified"
            return 1

    if not os.path.exists(target_folder+"/.gut"):
        os.makedirs(target_folder+"/.gut")

    if not os.path.exists(target_folder + "/.gut/history"):
        file(target_folder + "/.gut/history", "w").write("")

    if url:
        file(target_folder + "/.gut/remote_url", "w").write(url+"\n")

    if user:
        file(target_folder + "/.gut/local_user", "w").write(user+"\n")


def fetch():
    remote_url = file(".gut/remote_url").read().strip()
    net_download(remote_url, "repo_id", ".gut/repo_id")
    sync_down()


def get_change_sets(count):
    # TODO: only read the end part of the file?
    commits = []
    for commit in read_history(".gut/history"):
        commits.append(commit)
        if count > 0 and len(commits) > count:
            commits.pop(0)
    for commit in commits:
        print commit.to_gut()


def init(target_folder):
    target = target_folder
    if not os.path.exists(target):
        os.makedirs(target)
    file(target + "/repo_id", "w").write(random_hash())
    file(target + "/history", "w").write("")
    if not os.path.exists(target + "/objects"):
        os.mkdir(target + "/objects")
        for d in ["%02x" % n for n in range(0, 256)]:
            os.mkdir(target + "/objects/" + d)


def main(argv):
    parser = argparse.ArgumentParser(description='A file sync tool')
    subparsers = parser.add_subparsers(help='sub-command help')

    parser_init = subparsers.add_parser('init', help='create a blank server-side repository')
    parser_init.add_argument("target_folder")
    parser_init.set_defaults(func=init)

    parser_configure = subparsers.add_parser('configure', help='configure a checkout')
    parser_configure.add_argument("target_folder")
    parser_configure.add_argument("--url")
    parser_configure.add_argument("--user")
    parser_configure.set_defaults(func=configure)

    parser_fetch = subparsers.add_parser('fetch', help='initially fetch a remote repo')
    parser_fetch.set_defaults(func=fetch)

    parser_identifier = subparsers.add_parser('identifier', help='print the repository ID')
    parser_identifier.set_defaults(func=identifier)

    parser_size = subparsers.add_parser('size', help='print the size of the checkout')
    parser_size.set_defaults(func=size)

    parser_history_size = subparsers.add_parser('history-size', help='print the size of the metadata')
    parser_history_size.set_defaults(func=history_size)

    parser_current_revision = subparsers.add_parser('current-revision', help='print the current checkout revision ID')
    parser_current_revision.set_defaults(func=current_revision)

    parser_has_remote_changes = subparsers.add_parser('has-remote-changes', help='see if the repository has updates')
    parser_has_remote_changes.set_defaults(func=has_remote_changes)

    parser_has_local_changes = subparsers.add_parser('has-local-changes', help='see if the local files have been changed')
    parser_has_local_changes.set_defaults(func=has_local_changes)

    parser_sync_up = subparsers.add_parser('sync-up', help='push changes to the repository')
    parser_sync_up.set_defaults(func=sync_up)

    parser_sync_down = subparsers.add_parser('sync-down', help='pull changes from the repository')
    parser_sync_down.set_defaults(func=sync_down)

    parser_get_change_sets = subparsers.add_parser('get-change-sets', help='list recent changes')
    parser_get_change_sets.add_argument("--count", default=0, type=int)
    parser_get_change_sets.set_defaults(func=get_change_sets)

    args = parser.parse_args()
    if args.func:
        func = args.func
        del vars(args)['func']
        return func(**vars(args))


if __name__ == "__main__":
    sys.exit(main(sys.argv))
